{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFOTh-Ygt4Ud"
   },
   "source": [
    "#**SHL Grammar Rating Assignment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3982,
     "status": "ok",
     "timestamp": 1746445640459,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "hkY8V2r0QgpB",
    "outputId": "9b72e235-6626-4ca7-cb7d-096f6af08471"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1746445747491,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "aeuEnNnpRd5s",
    "outputId": "14e63d99-a524-4822-a54b-6f2afcee8dda"
   },
   "outputs": [],
   "source": [
    "!nbstripout SHL_Grammar_Rating.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q7qkEV2uIG7"
   },
   "source": [
    "Mounting Google Drives to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23181,
     "status": "ok",
     "timestamp": 1746383820554,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "_Au0yE5UkUaL",
    "outputId": "7daf1ce5-2c3d-4229-c616-e0f410cbad44"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh6qga3auRdE"
   },
   "source": [
    "##Preprocessing the training audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0frJwedFueZ4"
   },
   "source": [
    "Importing the necessary packages and preprocessing the training audio and saving them in new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "28e5abf8324b4ff182e458cebd556a20",
      "8c5f8430d592466db908d8a16b1d4b1e",
      "d40c838cc88343158be799dac7c30dfa",
      "0f620e437c0c4d8795a3cf951ce2b364",
      "9effd42506564485906d24126110f91f",
      "eb1b9db8a0934fd08a5d1472a11d9a9d",
      "0b15972b974948bc94ad4451c29a46fa",
      "4076336a1e884c559c5ece423a56287e",
      "cb28d870d9a5434a9031312450f26850",
      "e4557fec94c246d0a8a48c8f5d13b97c",
      "a5d95299077b41159051cb5d6a564a5f"
     ]
    },
    "executionInfo": {
     "elapsed": 491072,
     "status": "ok",
     "timestamp": 1746387217860,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "-IEqGIQqlfrk",
    "outputId": "12bd3a86-84ce-49c6-a313-f161655a4fa4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Paths\n",
    "AUDIO_DIR = '/content/drive/MyDrive/Colab Notebooks/audios/train'\n",
    "CSV_PATH = '/content/drive/MyDrive/Colab Notebooks/train.csv'\n",
    "PROCESSED_DIR = '/content/drive/MyDrive/Colab Notebooks/processed_audio'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
    "train_df.columns = ['filename', 'label']\n",
    "\n",
    "def preprocess_audio(file_path, save_path, sr=16000):\n",
    "    y, orig_sr = librosa.load(file_path, sr=None)\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y, orig_sr, sr)\n",
    "    y = y / max(abs(y))\n",
    "    y, _ = librosa.effects.trim(y, top_db=25)  #Silence Trimming\n",
    "    sf.write(save_path, y, sr)\n",
    "\n",
    "# Preprocessing each audio\n",
    "for filename in tqdm(train_df['filename']):\n",
    "    in_path = os.path.join(AUDIO_DIR, filename)\n",
    "    out_path = os.path.join(PROCESSED_DIR, filename)\n",
    "    preprocess_audio(in_path, out_path)\n",
    "\n",
    "print(\"‚úÖ Audio preprocessing completed. Files saved in:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYqcI6uyusbc"
   },
   "source": [
    "Verifying the Preprocessed audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1746387274109,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "LwU3vx-EnCUn",
    "outputId": "d489902d-1fac-443f-9447-65586fe3235a"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/content/drive/MyDrive/Colab Notebooks/processed_audio')\n",
    "print(f\"üîé Found {len(files)} preprocessed audio files.\\nExample files:\\n\", files[:5])\n",
    "\n",
    "# Checking sample rate and duration of a random file\n",
    "sample_file = os.path.join('/content/drive/MyDrive/Colab Notebooks/processed_audio', files[0])\n",
    "y, sr = librosa.load(sample_file, sr=None)\n",
    "\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "print(f\"üìÅ Sample file: {files[0]}\")\n",
    "print(f\"üïí Duration: {duration:.2f} seconds\")\n",
    "print(f\"üéß Sample rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYKcO7r_u486"
   },
   "source": [
    "Importing NLP Package and also making sure it is using GPU Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746387277450,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "eQJdk4hin53R",
    "outputId": "49b97f7e-c641-4a1a-bf15-8aaf117721bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwjls_-Dw0ph"
   },
   "source": [
    "Installing OpenAI's Whisper Package for trascripting the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98001,
     "status": "ok",
     "timestamp": 1746386701054,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "a4v18sEmok2l",
    "outputId": "b20aa2ab-4632-45d2-fa28-09af132056b0"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGeQ_1IbxJxO"
   },
   "source": [
    "Transcripting the audio files and exporting a CSV file of transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6ZXlSN1zMrl"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
    "train_df.columns = ['filename', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "9359e39f0fef4468bae6465635e4cf02",
      "648f451ec7bd40a991904550d9702180",
      "817043f5aad04632839002d0426a7431",
      "675e71ae45b143798c010f0eaa45355f",
      "1be30136fdc747749c616fbd03cd370d",
      "c4c815755cb64830bc85c3c2bfcfd8ec",
      "ab5d4661e2bf4fc0852c53019b40b12b",
      "dd769ed2093345f3b929e3e8956e6323",
      "162cbf0399d04f7f9587c586dab31798",
      "482e580b136846ef91f528c1ea0171bd",
      "bd6585cbfd1e4df19b26cdd1e3102094"
     ]
    },
    "executionInfo": {
     "elapsed": 1506805,
     "status": "ok",
     "timestamp": 1746388901172,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "FLRSg0CJn7-2",
    "outputId": "1ddc6d9d-5b5b-467b-d5e7-bcd06a32d15d"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Loading Whisper ASR model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Transcribing and collecting text\n",
    "transcripts = []\n",
    "\n",
    "for fname in tqdm(train_df['filename']):\n",
    "    audio_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/processed_audio', fname)\n",
    "    result = model.transcribe(audio_path, language='en')\n",
    "    transcripts.append(result['text'])\n",
    "\n",
    "# Adding transcripts to dataframe\n",
    "train_df['transcript'] = transcripts\n",
    "\n",
    "# Saving updated CSV\n",
    "train_df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_with_transcripts.csv', index=False)\n",
    "print(\"‚úÖ Transcriptions saved to: /content/drive/MyDrive/Colab Notebooks/temp_storage/train_with_transcripts.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4504vy4_xYet"
   },
   "source": [
    "Verifying the Transcripts CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1746388977402,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "O13xgJ39ocwu",
    "outputId": "af09f511-1a3d-401b-a29e-5d2d45438fbe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_with_transcripts.csv')\n",
    "print(\"üßæ Columns:\", df.columns.tolist())\n",
    "print(\"‚úÖ Total records:\", len(df))\n",
    "print(\"üó£ Sample transcript:\\n\")\n",
    "print(df[['filename', 'label', 'transcript']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Udm1jQlrxcgi"
   },
   "source": [
    "Checking for empty transcript of any audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746388987793,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "MxzbReYFq5Ta",
    "outputId": "fa70a000-aed2-4cd9-b1d1-351fd27fc402"
   },
   "outputs": [],
   "source": [
    "empty_transcripts = df['transcript'].str.strip().eq('').sum()\n",
    "print(f\"‚ö†Ô∏è Empty transcripts found: {empty_transcripts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzE84g0-xmrQ"
   },
   "source": [
    "Importing regular expression package to remove amiguity present in audio transcripts like \"uh\", \"like\", \"you know\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1746389083706,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "FdDuHityq8ot",
    "outputId": "a7ac08d7-d17c-4d5d-c88d-101182e2cfe4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Ambuiguious filters\n",
    "FILLERS = ['uh', 'um', 'erm', 'you know', 'like', 'i mean', 'hmm', 'ah', 'uhh', 'huh', 'duh', 'ohh', 'oh']\n",
    "\n",
    "def clean_transcript(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(?:' + '|'.join(FILLERS) + r')\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s([?.!,\"])', r'\\1', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Loading previous data\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_with_transcripts.csv')\n",
    "\n",
    "# Cleaning all transcripts\n",
    "df['cleaned_transcript'] = df['transcript'].astype(str).apply(clean_transcript)\n",
    "\n",
    "# Saving new version\n",
    "df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_cleaned.csv', index=False)\n",
    "print(\"‚úÖ Cleaned transcripts saved to: /content/drive/MyDrive/Colab Notebooks/temp_storage/train_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzT-EyQlx6gF"
   },
   "source": [
    "Sample Output for Cleaned Transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746389088690,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "hYbgI78lrCL5",
    "outputId": "4c69b6d2-3bf7-480d-fa5c-825eca33e27b"
   },
   "outputs": [],
   "source": [
    "print(df[['transcript', 'cleaned_transcript']].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfxjOsMBEDO5"
   },
   "source": [
    "##Feature Extraction on Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-kh60qLELFd"
   },
   "source": [
    "Grammar Feature Extraction for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "69e408415e1c47e89965e1884cdbdf00",
      "a6da8d95f0f048acb1e4eaa356b965e7",
      "9c7e22f84a604d27b6dfa536c1193393",
      "9f1ab02aee8048e4a280ada8251fc903",
      "557a54f9b10748c38fc523fa2ddd1b53",
      "67ee0ace09b540a28f1fbe52cb225aac",
      "c224a897676f443ea74b48f0c1d50b5e",
      "35823ac45122449c95e2357e3da78e07",
      "9ee8c1241413468ea5453508a6eb432c",
      "2f12f27b75de4736a549666edd898411",
      "4c43f569cf664ab9969a18d1e45b1b4a"
     ]
    },
    "executionInfo": {
     "elapsed": 753713,
     "status": "ok",
     "timestamp": 1746392922929,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "z_drsi4SCZEb",
    "outputId": "51160f3b-9862-4a36-e126-ea84d54542ec"
   },
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Grammar checker and NLP parser\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_cleaned.csv')\n",
    "\n",
    "# Feature addition lists\n",
    "error_counts = []\n",
    "avg_sent_lengths = []\n",
    "pos_diversities = []\n",
    "\n",
    "for text in tqdm(df['cleaned_transcript']):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        error_counts.append(0)\n",
    "        avg_sent_lengths.append(0)\n",
    "        pos_diversities.append(0)\n",
    "        continue\n",
    "\n",
    "    matches = tool.check(text)\n",
    "    error_counts.append(len(matches))\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sent_lengths = [len(sent) for sent in doc.sents]\n",
    "    pos_tags = [token.pos_ for token in doc if token.pos_ != 'SPACE']\n",
    "\n",
    "    avg_sent_lengths.append(sum(sent_lengths) / len(sent_lengths) if sent_lengths else 0)\n",
    "    pos_diversities.append(len(set(pos_tags)))\n",
    "\n",
    "# Appending new features\n",
    "df['grammar_errors'] = error_counts\n",
    "df['avg_sentence_length'] = avg_sent_lengths\n",
    "df['pos_diversity'] = pos_diversities\n",
    "\n",
    "# Saving the new file\n",
    "df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_features.csv', index=False)\n",
    "print(\"‚úÖ Grammar features saved to: /content/drive/MyDrive/Colab Notebooks/temp_storage/train_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlilkaVGcbL_"
   },
   "source": [
    "Adding more features to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746393002400,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "RcCXF24sEoaw",
    "outputId": "91e8c8bd-0b63-4f4b-ae2e-2efe6bd3da1c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_features.csv')\n",
    "\n",
    "df['word_count'] = df['cleaned_transcript'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df['grammar_errors_per_word'] = df['grammar_errors'] / df['word_count'].replace(0, 1)\n",
    "\n",
    "df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_features_enhanced.csv', index=False)\n",
    "print(\"Added word_count and grammar_errors_per_word as features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgeJv5ZAczu6"
   },
   "source": [
    "Installing Transformer Package for Grammmar Error feature addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5300,
     "status": "ok",
     "timestamp": 1746393027846,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "XE-WFqhtINR_",
    "outputId": "8db9f810-87a7-4bcb-bcf5-40b9d2c50c11"
   },
   "outputs": [],
   "source": [
    "!pip install happytransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytv6tbZbdPtr"
   },
   "source": [
    "Adding the GEC features to final ML model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449,
     "referenced_widgets": [
      "553919a6314d4837a3cf0db07a8df2ef",
      "156f05ff92294f1188ca1a768256f19d",
      "daf9b933f79e4e52bc89dccd188b5445",
      "e2731582759b48c996de3d805ada5720",
      "6aba17629257450f9ee3ce4ceb72ee6a",
      "7a5c30e366ba404395da7a426ebb060a",
      "5327fcaecdab47b48c94e55b6e693368",
      "b6aa4b46a9494e87b018d8e85bb7faa0",
      "941905b058ec40139597fe6c01955a2d",
      "046073f3f99b4e089007c2fc137fb909",
      "9a972208a57d441d913ecc4364439f2a",
      "b6f05704a79e4922bf18e6f840b84041",
      "bf8a38e4b83a4154a10b4973b13f6f80",
      "7e5f1647541c4e039d4a202b55a97460",
      "2a23d7b253e945a2a303796cc2d1fae1",
      "56b967114d1944ea8c21533df7c6008d",
      "471098a8e2ab4d25aaa14cd27f1932f2",
      "76dd1c8418804117aae4bd938f5b32e6",
      "8cd7164b646b40f996980533462651e8",
      "8e00f7d747fb4e6d9aebc10b4a0bea09",
      "7ff2a218436444a7bf4dbc5b643ef0a0",
      "01181816ef774b81890780eabc756a87",
      "694cc5209c684ee2971d9b460646ec83",
      "d6739bc668574f93a79732f4be1c48ea",
      "e423d78d9f4f4c3b825a692edbe070c9",
      "d21131a8f85c4959847c2c65f4105292",
      "5216027a8f3d43939fcca227657bd57c",
      "9b90111163904427b3998d321aad8143",
      "0ac066b682e34c30a1404e8c2cfc68db",
      "aefacb2b3c8c41cb957dc6fcb0d80983",
      "37236ac3ba514dff871f0b4e912db541",
      "8686a4e6efc8483f8af89a5ab921c8ce",
      "fc56d69da05c4ff0ad7c9c442b0fe890",
      "614a650db382463593a260f35c31a696",
      "ea086d05e60c453e83d0781b795e66bf",
      "09ee0262a4604e7aa88eaacfecb760d3",
      "b19c183fbf414a42873bc4970e94d645",
      "e079f20429e141d5a701a28bc3683483",
      "672c27ab0d7445619a573f33015b5d4e",
      "89854b02521f44b3b4dbebffa71f4368",
      "9e9b7bd5278b4ba2a55aff4fd74e48fd",
      "6ed5a111498d44b1967107be00c0ce9c",
      "a4d4447b19a5400c861a81d05ae4ceb8",
      "016cc7449db94aacb74321f591dd56e3",
      "afd9eb65dfc84515a0eab50b695c13d2",
      "8e785043014b48f3ba329fe489573a59",
      "a52dbf015b724299983040ace9afa165",
      "a91be11cacd84189843dd7b5f03f4439",
      "6c53125dbad34e3aad30f6f23f5441c4",
      "48bd4373ee0b483a975e203546cffc91",
      "ad1cfc79704743e88580ff2b787e98e2",
      "fe6dfb94fba140f3a7e65f391d4a9eec",
      "2c4b665d19d1471a9a1037ba55556404",
      "2c02bbdc9047448d8f387e1cde5850f5",
      "4c283e5ab264450f954d3273ebc6809b",
      "ae6657b9d7eb458491a603949fc7729f",
      "b0d0190a65244fc783dec9de5a54151a",
      "b38e93a882b148e2a6df65e4a4ed1473",
      "e1bbaec9d485406ca2fbbfa6092a2bd6",
      "74ecaa493050493e97268121f43c656e",
      "2fbaa91b9e7c4c7fafa0db0fbd7e7cfe",
      "61eefda0a3e54b068b23c2040a53476e",
      "e69ebd879bf9474f91671beeeefa8409",
      "83f528e506924f9bbfbdd56af01b3e3f",
      "7b9bc5ad78984939bbbd283edbf61a24",
      "6df1134620f646d98b04f56cbe03cf52",
      "a3adf0f84c39453eaadd517f2b62dbb7",
      "461c16377adb47f68350a32989804212",
      "b16ea1070c7849ffb503074a60b8bd93",
      "32d7d02741064c699f0c456c57564254",
      "1cc28b0ffaa84f139ca692958a0353c9",
      "e6ccc970580048e99f31b4df8d60e536",
      "4ffeffeba4ef4b2690f2f9d75ab7268a",
      "382ff8b4c10248a78f76b3331b9308e6",
      "dacd1953ac9d40379d71245694401424",
      "b69d2a0f49a44a7bba80b38dbad23784",
      "044e9c8465e441ff9ee8c7cbc59c439a",
      "6f3ecfb4d5d34a559bc4db2e2d3bed84",
      "f9c5ebcd23fd49b5a06d6aa510ede7ee",
      "6b2298dae9a345888676bb3f17d6f170",
      "5293772845104c9b8be9594c2fe10b28",
      "eed65b6b014c4bc2ae403a22f6c561c3",
      "3758d3a9fa9b4c4fb2d537658883a5af",
      "caac99fde4154ab29b90cfb974623a37",
      "e3c66446c45e49fe9096352d910d41b9",
      "5aa09384ef9646029b5d4a0f3907cb2d",
      "c0da0d25fef04b0c843c1b420e938ca9",
      "30293da61ac449a9936aacab7cfd7fea"
     ]
    },
    "executionInfo": {
     "elapsed": 483368,
     "status": "ok",
     "timestamp": 1746393588338,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "VLr8gL9OIr5m",
    "outputId": "668d2068-bb3b-4281-8fd2-673e564ae8b2"
   },
   "outputs": [],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_cleaned.csv')\n",
    "texts = df['cleaned_transcript'].astype(str).tolist()\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "edit_counts = []\n",
    "edit_ratios = []\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    result = happy_tt.generate_text(\"grammar: \" + text, args=args)\n",
    "    corrected = result.text\n",
    "\n",
    "    original_words = text.split()\n",
    "    corrected_words = corrected.split()\n",
    "    edits = sum(1 for o, c in zip(original_words, corrected_words) if o != c)\n",
    "    edits += abs(len(original_words) - len(corrected_words))\n",
    "\n",
    "    edit_counts.append(edits)\n",
    "    edit_ratios.append(edits / max(1, len(original_words)))\n",
    "\n",
    "# Adding new features to dataframe\n",
    "df['gec_edits'] = edit_counts\n",
    "df['gec_edit_rate'] = edit_ratios\n",
    "\n",
    "# Saving the updated CSV file\n",
    "df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_gec_features.csv', index=False)\n",
    "print(\"GEC features saved to /content/drive/MyDrive/Colab Notebooks/temp_storage/train_gec_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjFUC5sJdaao"
   },
   "source": [
    "Snippet of the earlier features incorporated into dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1746393651051,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "bfPCRenCJADY",
    "outputId": "725cd8c0-788b-4e02-cf82-5af926d2b81e"
   },
   "outputs": [],
   "source": [
    "archis=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_features.csv')\n",
    "archis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrGbkXeSduFk"
   },
   "source": [
    "Combining of the earlier engineered general feature and GEC features into final dataset ready for ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1746394013993,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "sgahxmIaMT50",
    "outputId": "be3e29bb-4348-4b86-8622-a89be81fc9ce"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_main = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_features_enhanced.csv')\n",
    "df_gec = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_gec_features.csv')\n",
    "\n",
    "df_combined = df_main.copy()\n",
    "df_combined['gec_edits'] = df_gec['gec_edits']\n",
    "df_combined['gec_edit_rate'] = df_gec['gec_edit_rate']\n",
    "\n",
    "df_combined.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_all_features.csv', index=False)\n",
    "print(\"‚úÖ Combined feature set saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22RgIci3eAwT"
   },
   "source": [
    "###ML Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4V7dPJPeIpG"
   },
   "source": [
    "Applying various Machine Learning Algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746445317984,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "P5ZLslMAP5UP",
    "outputId": "6fde930c-8f10-4d67-bdd7-a894c7b726fd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Try to get the name of the currently open notebook\n",
    "notebook_name = [f for f in os.listdir('.') if f.endswith('.ipynb')]\n",
    "\n",
    "if notebook_name:\n",
    "    notebook_name = notebook_name[0]\n",
    "    print(f\"Attempting to load notebook: {notebook_name}\")\n",
    "    try:\n",
    "        with open(notebook_name, 'r') as f:\n",
    "            notebook_data = json.load(f)\n",
    "        print(\"Notebook data loaded successfully.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding notebook JSON: {e}\")\n",
    "        print(\"There might be an issue with the notebook file's structure.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{notebook_name}' not found.\")\n",
    "else:\n",
    "    print(\"Error: Could not determine the name of the current notebook file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1746394064328,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "LFiCxsuhLFVv",
    "outputId": "49d3a587-2cdb-4771-90a6-70cdacc8b8b9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/train_all_features.csv')\n",
    "\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "feature_cols = [\n",
    "    'grammar_errors',\n",
    "    'avg_sentence_length',\n",
    "    'pos_diversity',\n",
    "    'word_count',\n",
    "    'grammar_errors_per_word',\n",
    "    'gec_edits',\n",
    "    'gec_edit_rate'\n",
    "]\n",
    "X = df[feature_cols]\n",
    "y = df['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    corr, _ = pearsonr(y_val, preds)\n",
    "    results[name] = {'RMSE': rmse, 'Pearson': corr}\n",
    "    print(f\"{name} -> RMSE: {rmse:.4f}, Pearson: {corr:.4f}\")\n",
    "\n",
    "best_model = max(results.items(), key=lambda x: x[1]['Pearson'])\n",
    "print(f\"\\n‚úÖ Best model: {best_model[0]} with Pearson correlation: {best_model[1]['Pearson']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJgg13ufedFG"
   },
   "source": [
    "Best Model was GradientBoosting with Pearson Correlation of 0.3167. Hence training the Model with GradientBoosting with better hyper parametering to get refined Pearson Correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366318,
     "status": "ok",
     "timestamp": 1746395923444,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "Q4j6p5GaNL3-",
    "outputId": "4094c108-b2db-4e25-9805-99fb2f0c6a9e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting stages (trees)\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinking\n",
    "    'max_depth': [3, 5, 7],  # Depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used for fitting each tree\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f\"Optimized GradientBoosting -> RMSE: {rmse:.4f}, Pearson: {pearson_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtzaE3b3rGie"
   },
   "source": [
    "##Pre-Processing the test audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLeS9fdCyQYc"
   },
   "source": [
    "Following same procedure as train audio files. Preprocessing the test audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34159,
     "status": "ok",
     "timestamp": 1746397582686,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "M1CxGlW6rLsp",
    "outputId": "7b49556f-0a6a-40dc-d078-2a0bca3dab33"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_AUDIO_DIR = '/content/drive/MyDrive/Colab Notebooks/audios/test'\n",
    "TEST_CSV_PATH = '/content/drive/MyDrive/Colab Notebooks/test.csv'\n",
    "TEST_PROCESSED_DIR = '/content/drive/MyDrive/Colab Notebooks/processed_audio_test'\n",
    "os.makedirs(TEST_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "def preprocess_audio(file_path, save_path, sr=16000):\n",
    "    y, orig_sr = librosa.load(file_path, sr=None)\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y, orig_sr, sr)\n",
    "    y = y / max(abs(y))\n",
    "    y, _ = librosa.effects.trim(y, top_db=25)\n",
    "    sf.write(save_path, y, sr)\n",
    "\n",
    "for filename in tqdm(test_df['filename']):\n",
    "    in_path = os.path.join(TEST_AUDIO_DIR, filename)\n",
    "    out_path = os.path.join(TEST_PROCESSED_DIR, filename)\n",
    "    preprocess_audio(in_path, out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3J7uFbvyfSv"
   },
   "source": [
    "Transcripting the Test Audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 736911,
     "status": "ok",
     "timestamp": 1746398788994,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "8SkXcE5_rpN7",
    "outputId": "0a3e5d26-6e5f-44cb-a6b9-d52b1ba67ee4"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model_whisper = whisper.load_model(\"base\")\n",
    "transcripts = []\n",
    "\n",
    "for fname in tqdm(test_df['filename']):\n",
    "    audio_path = os.path.join(TEST_PROCESSED_DIR, fname)\n",
    "    result = model_whisper.transcribe(audio_path, language='en')\n",
    "    transcripts.append(result['text'])\n",
    "\n",
    "test_df['transcript'] = transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak2rVGR3yjHJ"
   },
   "source": [
    "Removing ambiguity in transcript of test audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746398930395,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "bfp1BhYtrtnY",
    "outputId": "0591c00f-6196-4da2-d418-c8c4c56429a0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FILLERS = ['uh', 'um', 'erm', 'you know', 'like', 'i mean', 'hmm', 'ah', 'uhh', 'huh']\n",
    "\n",
    "def clean_transcript(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(?:' + '|'.join(FILLERS) + r')\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s([?.!,\"])', r'\\1', text)\n",
    "    return text.strip()\n",
    "\n",
    "test_df['cleaned_transcript'] = test_df['transcript'].apply(clean_transcript)\n",
    "\n",
    "test_df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/test_cleaned.csv', index=False)\n",
    "print(\"Cleaned test transcripts saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44NPXzVQT0xI"
   },
   "source": [
    "Test Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322401,
     "status": "ok",
     "timestamp": 1746399261107,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "FIoZXNK_P9Qk",
    "outputId": "9eab8630-fc27-4282-f510-da98ba8a3d92"
   },
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "import spacy\n",
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "error_counts = []\n",
    "avg_sent_lengths = []\n",
    "pos_diversities = []\n",
    "gec_edits = []\n",
    "gec_rates = []\n",
    "word_counts = []\n",
    "\n",
    "for text in tqdm(test_df['cleaned_transcript']):\n",
    "    matches = tool.check(text)\n",
    "    error_counts.append(len(matches))\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sent_lens = [len(sent) for sent in doc.sents]\n",
    "    pos_tags = [token.pos_ for token in doc if token.pos_ != 'SPACE']\n",
    "    avg_sent_lengths.append(sum(sent_lens) / len(sent_lens) if sent_lens else 0)\n",
    "    pos_diversities.append(len(set(pos_tags)))\n",
    "\n",
    "    words = text.split()\n",
    "    word_counts.append(len(words))\n",
    "\n",
    "    corrected = happy_tt.generate_text(\"grammar: \" + text, args=args).text\n",
    "    edits = sum(1 for o, c in zip(words, corrected.split()) if o != c)\n",
    "    edits += abs(len(words) - len(corrected.split()))\n",
    "    gec_edits.append(edits)\n",
    "    gec_rates.append(edits / max(1, len(words)))\n",
    "\n",
    "# Adding the same features as train dataset features\n",
    "test_df['grammar_errors'] = error_counts\n",
    "test_df['avg_sentence_length'] = avg_sent_lengths\n",
    "test_df['pos_diversity'] = pos_diversities\n",
    "test_df['word_count'] = word_counts\n",
    "test_df['grammar_errors_per_word'] = test_df['grammar_errors'] / test_df['word_count'].replace(0, 1)\n",
    "test_df['gec_edits'] = gec_edits\n",
    "test_df['gec_edit_rate'] = gec_rates\n",
    "\n",
    "test_df.to_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/test_cleaned.csv', index=False)\n",
    "print(\"‚úÖ Updated test_cleaned.csv with new features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn91dg3qfnVt"
   },
   "source": [
    "Testing the test data on the trained ML model and saving the result into submission.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746399658867,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "tldm9pydU07d",
    "outputId": "73449157-0ca6-4a85-91a2-7c106cd3f04f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/temp_storage/test_cleaned.csv')\n",
    "\n",
    "test_features = test_df[['grammar_errors', 'avg_sentence_length', 'pos_diversity',\n",
    "                         'word_count', 'grammar_errors_per_word', 'gec_edits', 'gec_edit_rate']]\n",
    "\n",
    "y_pred = best_gb.predict(test_features)\n",
    "\n",
    "y_pred_rounded = np.round(y_pred).astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': test_df['filename'],\n",
    "    'label': y_pred_rounded\n",
    "    })\n",
    "\n",
    "submission_df.to_csv('/content/drive/MyDrive/Colab Notebooks/submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9i4O_NwiEtq"
   },
   "source": [
    "##Final Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJdegdWSiK2B"
   },
   "source": [
    "Best Pearson Correlation Score achieved with GradientBoosting Model with value of 0.3167 which is almost producing correct grammatical labelling.\n",
    "\n",
    "Future ML Pipeline->\n",
    "  Adding more features into dataset using more efficient NLP libraries and perform better Hyper Parametering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6PV2T7kjEL5"
   },
   "source": [
    "Exporting the Model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746399999828,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "nrNoBUL2ZeTg",
    "outputId": "1f61562c-ba6a-4394-ca8b-ee58ef439f6c"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_gb, '/content/drive/MyDrive/Colab Notebooks/best_gradient_boosting_model.pkl')\n",
    "\n",
    "print(\"Model saved as best_gradient_boosting_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wZDf8OnjU0z"
   },
   "source": [
    "##Some Visualization to understand the Model better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym1SzX5SnRlH"
   },
   "source": [
    "Feature Importance using BarPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1746400159290,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "iUnCoNsXjTV9",
    "outputId": "b692d3b2-b037-4e90-bb68-a6719d329f7f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "feature_importances = best_gb.feature_importances_\n",
    "features = ['grammar_errors', 'avg_sentence_length', 'pos_diversity',\n",
    "            'word_count', 'grammar_errors_per_word', 'gec_edits', 'gec_edit_rate']\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_df, palette='viridis')\n",
    "plt.title('Feature Importance (GradientBoosting)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyHMGKUmndb3"
   },
   "source": [
    "Model Prediction Histographical Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1746400188356,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "QtZhJ1H6jurz",
    "outputId": "30ecd546-8bd2-46ae-c169-7a987054e81b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_pred, bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Model Predictions')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cgYAne5nloF"
   },
   "source": [
    "RMSE vs Pearson Correlation Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1746400561098,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "NKYwNfmWkBLz",
    "outputId": "c6d90e19-8bff-4019-c0be-c82406417aec"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Best model scores\n",
    "rmse_best = 1.1311\n",
    "pearson_best = 0.3176\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=['RMSE', 'Pearson Correlation'], y=[rmse_best, pearson_best], palette='muted')\n",
    "plt.title('Best Model Performance: RMSE & Pearson Correlation')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, max(rmse_best, pearson_best) + 0.5)  # Adjust y-axis for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ65BSTZnui1"
   },
   "source": [
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1746400759641,
     "user": {
      "displayName": "Anubhav Arya",
      "userId": "03314715707341155703"
     },
     "user_tz": -330
    },
    "id": "lg7a-XdQkGE6",
    "outputId": "327c935d-5ea2-4b18-ac2f-f8ec151e4545"
   },
   "outputs": [],
   "source": [
    "corr_matrix = test_df[['grammar_errors', 'avg_sentence_length', 'pos_diversity',\n",
    "                       'word_count', 'grammar_errors_per_word', 'gec_edits', 'gec_edit_rate']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmlkb4VbnD2E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOrHZiRQdeaOU172z83hkwH",
   "gpuType": "T4",
   "mount_file_id": "18P14fFwwbot6fzv-4TXwb2XMeZMkrFCL",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
